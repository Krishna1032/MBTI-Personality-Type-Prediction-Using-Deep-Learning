{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ff69146-42c9-44bd-93e3-54aab89e50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b01eb7e-1aab-4716-b9f2-d5209915dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from transformers import TFBertModel\n",
    "\n",
    "# Define the custom BertLayer\n",
    "class BertLayer(layers.Layer):\n",
    "    def __init__(self, pretrained_model_name, **kwargs):\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        self.bert = TFBertModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask, token_type_ids = inputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        return outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f04afb8-e1b1-44e6-bc5a-09ead7344827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "class MBTIPredictor:\n",
    "    def __init__(self, model_path, tokenizer_name='bert-large-uncased', max_length=500):\n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load the model\n",
    "        self.model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects={'BertLayer': BertLayer}  \n",
    "        )\n",
    "        \n",
    "        # MBTI traits\n",
    "        self.traits = ['IE', 'NS', 'TF', 'JP']\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        # Tokenize the input text\n",
    "        tokenized_data = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='np'\n",
    "        )\n",
    "        return {\n",
    "            \"input_word_ids\": tokenized_data['input_ids'],\n",
    "            \"attention_mask\": tokenized_data['attention_mask'],\n",
    "            \"token_type_ids\": tokenized_data['token_type_ids']\n",
    "        }\n",
    "\n",
    "    def interpret_predictions(self, predictions):\n",
    "        personality = []\n",
    "        for trait, output in zip(self.traits, predictions):\n",
    "            score = output[0]  # Single input prediction\n",
    "            if score > 0.5:\n",
    "                personality.append(trait[0])  # First letter (e.g., 'I' for 'IE')\n",
    "            else:\n",
    "                personality.append(trait[1])  # Second letter (e.g., 'E' for 'IE')\n",
    "        return ''.join(personality)\n",
    "\n",
    "    def predict(self, text):\n",
    "        # Full pipeline: tokenize, predict, interpret\n",
    "        tokenized_input = self.tokenize_text(text)\n",
    "        predictions = self.model.predict(tokenized_input)\n",
    "        return self.interpret_predictions(predictions)\n",
    "\n",
    "predictor = MBTIPredictor(model_path=\"saved_models/bert_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56551815-81f2-45e9-b954-c01f97d99e74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_2\" is incompatible with the layer: expected shape=(None, 128), found shape=(1, 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Getting started with data science and applying machine learning has never been as simple as it is now. There are many free and paid online tutorials and courses out there to help you to get started. I’ve recently started to learn, play, and work on Data Science & Machine Learning on Kaggle.com. In this brief post, I’d like to share my experience with the Kaggle Python Docker image, which simplifies the Data Scientist’s life.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mAwesome #AWS monitoring introduction.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m predicted_mbti \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted MBTI Personality: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_mbti\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 47\u001b[0m, in \u001b[0;36mMBTIPredictor.predict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Full pipeline: tokenize, predict, interpret\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     tokenized_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_text(text)\n\u001b[0;32m---> 47\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpret_predictions(predictions)\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/keras/src/layers/input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional_2\" is incompatible with the layer: expected shape=(None, 128), found shape=(1, 500)"
     ]
    }
   ],
   "source": [
    "input_text = (\n",
    "    \"\"\"Getting started with data science and applying machine learning has never been as simple as it is now. There are many free and paid online tutorials and courses out there to help you to get started. I’ve recently started to learn, play, and work on Data Science & Machine Learning on Kaggle.com. In this brief post, I’d like to share my experience with the Kaggle Python Docker image, which simplifies the Data Scientist’s life.\n",
    "Awesome #AWS monitoring introduction.\n",
    "HPE Software (now @MicroFocusSW) won the platinum reader's choice #ITAWARDS 2017 in the new category #CloudMonitoring\n",
    "Certified as AWS Certified Solutions Architect \n",
    "Hi, please have a look at my Udacity interview about online learning and machine learning,\n",
    "Very interesting to see the  lessons learnt during the HP Operations Orchestration to CloudSlang journey. http://bit.ly/1Xo41ci \n",
    "I came across a post on devopsdigest.com and need your input: “70% DevOps organizations Unhappy with DevOps Monitoring Tools”\n",
    "In a similar investigation I found out that many DevOps organizations use several monitoring tools in parallel. Senu, Nagios, LogStach and SaaS offerings such as DataDog or SignalFX to name a few. However, one element is missing: Consolidation of alerts and status in a single pane of glass, which enables fast remediation of application and infrastructure uptime and performance issues.\n",
    "Sure, there are commercial tools on the market for exactly this use case but these tools are not necessarily optimized for DevOps.\n",
    "So, here my question to you: In your DevOps project, have you encountered that the lack of consolidation of alerts and status is a real issue? If yes, how did you approach the problem? Or is an ChatOps approach just right?\n",
    "You will probably hear more and more about ChatOps - at conferences, DevOps meet-ups or simply from your co-worker at the coffee station. ChatOps is a term and concept coined by GitHub. It's about the conversation-driven development, automation, and operations.\n",
    "Now the question is: why and how would I, as an ops-focused engineer, implement and use ChatOps in my organization? The next question then is: How to include my tools into the chat conversation?\n",
    "Let’s begin by having a look at a use case. The Closed Looped Incidents Process (CLIP) can be rejuvenated with ChatOps. The work from the incident detection runs through monitoring until the resolution of issues in your application or infrastructure can be accelerated with improved, cross-team communication and collaboration.\n",
    "In this blog post, I am going to describe and share my experience with deploying HP Operations Manager i 10.0 (OMi) on HP Helion Public Cloud. An Infrastructure as a Service platform such as HP Helion Public Cloud Compute is a great place to quickly spin-up a Linux server and install HP Operations Manager i for various use scenarios. An example of a good use case is monitoring workloads across public clouds such as AWS and Azure.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "predicted_mbti = predictor.predict(input_text)\n",
    "print(f\"Predicted MBTI Personality: {predicted_mbti}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee7fc2-9182-4a99-bec4-7d9ac767db97",
   "metadata": {},
   "source": [
    "## Model Using LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98166028-91ab-4c0e-9e8f-8600f35017dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load the saved model\n",
    "model_path = \"saved_models/LSTM_based_model.keras\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Recreate the TextVectorization layer\n",
    "max_tokens = 10_000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens = 10_000,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = 250\n",
    ")\n",
    "\n",
    "# Load the vocabulary from the saved file\n",
    "vocab_path = \"saved_models/vectorizer_vocab.txt\"\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    vocabulary = [line.strip() for line in f.readlines()]\n",
    "\n",
    "vectorizer.set_vocabulary(vocabulary)\n",
    "\n",
    "\n",
    "def predict_mbti(input_text):\n",
    "    input_tensor = tf.convert_to_tensor([input_text])\n",
    "    predictions = model.predict(input_tensor)\n",
    "\n",
    "    # Interpret predictions\n",
    "    def interpret_predictions(predictions):\n",
    "        traits = ['IE', 'NS', 'TF', 'JP']\n",
    "        personality = []\n",
    "        for trait, output in zip(traits, predictions):\n",
    "            score = output[0]  # Single prediction for each output\n",
    "            personality.append(trait[0] if score > 0.5 else trait[1])\n",
    "        return ''.join(personality)\n",
    "\n",
    "    return interpret_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b714847-0e8e-472c-b1ad-ae0dd5ee6da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step\n",
      "Predicted MBTI Personality: INTP\n"
     ]
    }
   ],
   "source": [
    "input_text = (\"\"\"Getting started with data science and applying machine learning has never been as simple as it is now. There are many free and paid online tutorials and courses out there to help you to get started. I’ve recently started to learn, play, and work on Data Science & Machine Learning on Kaggle.com. In this brief post, I’d like to share my experience with the Kaggle Python Docker image, which simplifies the Data Scientist’s life.\n",
    "Awesome #AWS monitoring introduction.\n",
    "HPE Software (now @MicroFocusSW) won the platinum reader's choice #ITAWARDS 2017 in the new category #CloudMonitoring\n",
    "Certified as AWS Certified Solutions Architect \n",
    "Hi, please have a look at my Udacity interview about online learning and machine learning,\n",
    "Very interesting to see the  lessons learnt during the HP Operations Orchestration to CloudSlang journey. http://bit.ly/1Xo41ci \n",
    "I came across a post on devopsdigest.com and need your input: “70% DevOps organizations Unhappy with DevOps Monitoring Tools”\n",
    "In a similar investigation I found out that many DevOps organizations use several monitoring tools in parallel. Senu, Nagios, LogStach and SaaS offerings such as DataDog or SignalFX to name a few. However, one element is missing: Consolidation of alerts and status in a single pane of glass, which enables fast remediation of application and infrastructure uptime and performance issues.\n",
    "Sure, there are commercial tools on the market for exactly this use case but these tools are not necessarily optimized for DevOps.\n",
    "So, here my question to you: In your DevOps project, have you encountered that the lack of consolidation of alerts and status is a real issue? If yes, how did you approach the problem? Or is an ChatOps approach just right?\n",
    "You will probably hear more and more about ChatOps - at conferences, DevOps meet-ups or simply from your co-worker at the coffee station. ChatOps is a term and concept coined by GitHub. It's about the conversation-driven development, automation, and operations.\n",
    "Now the question is: why and how would I, as an ops-focused engineer, implement and use ChatOps in my organization? The next question then is: How to include my tools into the chat conversation?\n",
    "Let’s begin by having a look at a use case. The Closed Looped Incidents Process (CLIP) can be rejuvenated with ChatOps. The work from the incident detection runs through monitoring until the resolution of issues in your application or infrastructure can be accelerated with improved, cross-team communication and collaboration.\n",
    "In this blog post, I am going to describe and share my experience with deploying HP Operations Manager i 10.0 (OMi) on HP Helion Public Cloud. An Infrastructure as a Service platform such as HP Helion Public Cloud Compute is a great place to quickly spin-up a Linux server and install HP Operations Manager i for various use scenarios. An example of a good use case is monitoring workloads across public clouds such as AWS and Azure.\n",
    "\"\"\")\n",
    "predicted_mbti = predict_mbti(input_text)\n",
    "print(f\"Predicted MBTI Personality: {predicted_mbti}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdc5eb-e9e4-479f-bb18-0957c3815fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
